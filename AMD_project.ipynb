{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e1fa7d8",
   "metadata": {},
   "source": [
    "# Market basket analysis: IMDb Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc279266",
   "metadata": {},
   "source": [
    "#### Lo Vecchio Gianluca (961536)\n",
    "##### DSE(2020-2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e202fbb7",
   "metadata": {},
   "source": [
    "## Download the dataset through Kaggle API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cf051d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in /opt/conda/lib/python3.7/site-packages (1.5.12)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kaggle) (2021.5.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.8.1)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from kaggle) (2.25.1)\n",
      "Requirement already satisfied: python-slugify in /opt/conda/lib/python3.7/site-packages (from kaggle) (5.0.2)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from kaggle) (4.61.1)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.7/site-packages (from kaggle) (1.26.5)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /opt/conda/lib/python3.7/site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->kaggle) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b18f03-4166-4394-b445-c1b3b15a3ac7",
   "metadata": {
    "tags": []
   },
   "source": [
    "!mkdir ~/.kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deff711-9db0-43d8-a00e-385930461591",
   "metadata": {},
   "source": [
    "*Is necessary to upload the kaggle.json file manualy in the enviroment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f93fc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp ~/kaggle.json ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b73f53b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /home/jupyter/.kaggle/kaggle.json'\n",
      "Downloading imdb-dataset.zip to /home/jupyter\n",
      " 98%|██████████████████████████████████████ | 1.41G/1.44G [00:05<00:00, 313MB/s]\n",
      "100%|███████████████████████████████████████| 1.44G/1.44G [00:05<00:00, 273MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download -d 'ashirwadsangwan/imdb-dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6c58e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  imdb-dataset.zip\n",
      "replace name.basics.tsv.gz? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!unzip \\*.zip && rm *.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0614af3",
   "metadata": {},
   "source": [
    "## Setting up the Jupyter Notebook environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3117480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /opt/conda/lib/python3.7/site-packages (3.1.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in /opt/conda/lib/python3.7/site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext,SparkConf\n",
    "conf = SparkConf().setAppName(\"MB Analysis\")\n",
    "conf = (conf.setMaster('local[*]')\n",
    "        .set('spark.executor.memory', '4G')\n",
    "        .set('spark.driver.memory', '45G')\n",
    "        .set('spark.driver.maxResultSize', '10G')\n",
    "        .set('spark.reducer.maxSizeInFlight', '96m'))\n",
    "sc = pyspark.SparkContext('local[*]', conf=conf)\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import collect_set, col, count\n",
    "spark = SparkSession.builder.appName(\"MB Analysis\").getOrCreate()\n",
    "import itertools\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0164a22b",
   "metadata": {},
   "source": [
    "## Loading the three datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eeddb4c",
   "metadata": {},
   "source": [
    "Importing the three necessary tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d57a2523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FilmTitle = spark.read.csv(\"title.basics.tsv\", sep=r'\\t', header=True)\n",
    "Job = spark.read.csv(\"title.principals.tsv\", sep=r'\\t', header=True)\n",
    "NamesCast = spark.read.csv(\"name.basics.tsv\", sep=r'\\t', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03151a2-8c5d-447f-9f1f-301add163be2",
   "metadata": {},
   "source": [
    "Film table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d30d4dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T16:11:55.047477Z",
     "iopub.status.busy": "2021-07-03T16:11:55.047104Z",
     "iopub.status.idle": "2021-07-03T16:11:55.282446Z",
     "shell.execute_reply": "2021-07-03T16:11:55.281207Z",
     "shell.execute_reply.started": "2021-07-03T16:11:55.047447Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000001|    short|          Carmencita|          Carmencita|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000002|    short|Le clown et ses c...|Le clown et ses c...|      0|     1892|     \\N|             5|     Animation,Short|\n",
      "|tt0000003|    short|      Pauvre Pierrot|      Pauvre Pierrot|      0|     1892|     \\N|             4|Animation,Comedy,...|\n",
      "|tt0000004|    short|         Un bon bock|         Un bon bock|      0|     1892|     \\N|            \\N|     Animation,Short|\n",
      "|tt0000005|    short|    Blacksmith Scene|    Blacksmith Scene|      0|     1893|     \\N|             1|        Comedy,Short|\n",
      "|tt0000006|    short|   Chinese Opium Den|   Chinese Opium Den|      0|     1894|     \\N|             1|               Short|\n",
      "|tt0000007|    short|Corbett and Court...|Corbett and Court...|      0|     1894|     \\N|             1|         Short,Sport|\n",
      "|tt0000008|    short|Edison Kinetoscop...|Edison Kinetoscop...|      0|     1894|     \\N|             1|   Documentary,Short|\n",
      "|tt0000009|    movie|          Miss Jerry|          Miss Jerry|      0|     1894|     \\N|            45|             Romance|\n",
      "|tt0000010|    short| Exiting the Factory|La sortie de l'us...|      0|     1895|     \\N|             1|   Documentary,Short|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FilmTitle.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c86d3e3-fd7a-4bd0-86f5-9e5fdba31c21",
   "metadata": {},
   "source": [
    "Job table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fdc9fc13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T16:11:59.462189Z",
     "iopub.status.busy": "2021-07-03T16:11:59.461721Z",
     "iopub.status.idle": "2021-07-03T16:11:59.610076Z",
     "shell.execute_reply": "2021-07-03T16:11:59.609049Z",
     "shell.execute_reply.started": "2021-07-03T16:11:59.462136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+---------------+--------------------+-----------+\n",
      "|   tconst|ordering|   nconst|       category|                 job| characters|\n",
      "+---------+--------+---------+---------------+--------------------+-----------+\n",
      "|tt0000001|       1|nm1588970|           self|                  \\N|[\"Herself\"]|\n",
      "|tt0000001|       2|nm0005690|       director|                  \\N|         \\N|\n",
      "|tt0000001|       3|nm0374658|cinematographer|director of photo...|         \\N|\n",
      "|tt0000002|       1|nm0721526|       director|                  \\N|         \\N|\n",
      "|tt0000002|       2|nm1335271|       composer|                  \\N|         \\N|\n",
      "|tt0000003|       1|nm0721526|       director|                  \\N|         \\N|\n",
      "|tt0000003|       2|nm5442194|       producer|            producer|         \\N|\n",
      "|tt0000003|       3|nm1335271|       composer|                  \\N|         \\N|\n",
      "|tt0000003|       4|nm5442200|         editor|                  \\N|         \\N|\n",
      "|tt0000004|       1|nm0721526|       director|                  \\N|         \\N|\n",
      "+---------+--------+---------+---------------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Job.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88569c13-ef53-4cf9-8f56-eeee9d2fee03",
   "metadata": {},
   "source": [
    "Actress and Actors table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d377aff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-03T16:12:02.226755Z",
     "iopub.status.busy": "2021-07-03T16:12:02.226393Z",
     "iopub.status.idle": "2021-07-03T16:12:02.352522Z",
     "shell.execute_reply": "2021-07-03T16:12:02.350655Z",
     "shell.execute_reply.started": "2021-07-03T16:12:02.226726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|   nconst|    primaryName|birthYear|deathYear|   primaryProfession|      knownForTitles|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "|nm0000001|   Fred Astaire|     1899|     1987|soundtrack,actor,...|tt0050419,tt00531...|\n",
      "|nm0000002|  Lauren Bacall|     1924|     2014|  actress,soundtrack|tt0071877,tt01170...|\n",
      "|nm0000003|Brigitte Bardot|     1934|       \\N|actress,soundtrac...|tt0054452,tt00491...|\n",
      "|nm0000004|   John Belushi|     1949|     1982|actor,writer,soun...|tt0077975,tt00725...|\n",
      "|nm0000005| Ingmar Bergman|     1918|     2007|writer,director,a...|tt0069467,tt00509...|\n",
      "|nm0000006| Ingrid Bergman|     1915|     1982|actress,soundtrac...|tt0038109,tt00368...|\n",
      "|nm0000007|Humphrey Bogart|     1899|     1957|actor,soundtrack,...|tt0043265,tt00338...|\n",
      "|nm0000008|  Marlon Brando|     1924|     2004|actor,soundtrack,...|tt0070849,tt00787...|\n",
      "|nm0000009| Richard Burton|     1925|     1984|actor,producer,so...|tt0057877,tt00878...|\n",
      "|nm0000010|   James Cagney|     1899|     1986|actor,soundtrack,...|tt0035575,tt00318...|\n",
      "+---------+---------------+---------+---------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "NamesCast.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d42d2c",
   "metadata": {},
   "source": [
    "## PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed1ecd",
   "metadata": {},
   "source": [
    "SparkSQL views are necessary to make queries and create new tables with joined data from the original ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4782a18e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FilmTitle.createOrReplaceTempView(\"FilmTitle\")\n",
    "Job.createOrReplaceTempView(\"Job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38be48",
   "metadata": {},
   "source": [
    "Get item (Actors and Actress) query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d92c0d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+---------+--------+---+--------------------+\n",
      "|   tconst|ordering|   nconst|category|job|          characters|\n",
      "+---------+--------+---------+--------+---+--------------------+\n",
      "|tt0000005|       1|nm0443482|   actor| \\N|      [\"Blacksmith\"]|\n",
      "|tt0000005|       2|nm0653042|   actor| \\N|       [\"Assistant\"]|\n",
      "|tt0000007|       1|nm0179163|   actor| \\N|                  \\N|\n",
      "|tt0000007|       2|nm0183947|   actor| \\N|                  \\N|\n",
      "|tt0000008|       1|nm0653028|   actor| \\N|    [\"Sneezing Man\"]|\n",
      "|tt0000009|       1|nm0063086| actress| \\N|[\"Miss Geraldine ...|\n",
      "|tt0000009|       2|nm0183823|   actor| \\N|    [\"Mr. Hamilton\"]|\n",
      "|tt0000009|       3|nm1309758|   actor| \\N|[\"Chauncey Depew ...|\n",
      "|tt0000011|       1|nm3692297|   actor| \\N|        [\"Acrobats\"]|\n",
      "|tt0000014|       1|nm0166380|   actor| \\N|    [\"The Gardener\"]|\n",
      "+---------+--------+---------+--------+---+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actors =  \"\"\" SELECT *\n",
    "              FROM Job\n",
    "              WHERE (category = 'actor') OR (category = 'actress')\n",
    "\"\"\"\n",
    "actors = spark.sql(actors)\n",
    "actors.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a7189",
   "metadata": {},
   "source": [
    "Get baskets (Movies) query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b1ecc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|   tconst|titleType|        primaryTitle|       originalTitle|isAdult|startYear|endYear|runtimeMinutes|              genres|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "|tt0000009|    movie|          Miss Jerry|          Miss Jerry|      0|     1894|     \\N|            45|             Romance|\n",
      "|tt0000147|    movie|The Corbett-Fitzs...|The Corbett-Fitzs...|      0|     1897|     \\N|            20|Documentary,News,...|\n",
      "|tt0000335|    movie|Soldiers of the C...|Soldiers of the C...|      0|     1900|     \\N|            \\N|     Biography,Drama|\n",
      "|tt0000502|    movie|            Bohemios|            Bohemios|      0|     1905|     \\N|           100|                  \\N|\n",
      "|tt0000574|    movie|The Story of the ...|The Story of the ...|      0|     1906|     \\N|            70|Biography,Crime,D...|\n",
      "|tt0000615|    movie|  Robbery Under Arms|  Robbery Under Arms|      0|     1907|     \\N|            \\N|               Drama|\n",
      "|tt0000630|    movie|              Hamlet|              Amleto|      0|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000675|    movie|         Don Quijote|         Don Quijote|      0|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000676|    movie|Don Álvaro o la f...|Don Álvaro o la f...|      0|     1908|     \\N|            \\N|               Drama|\n",
      "|tt0000679|    movie|The Fairylogue an...|The Fairylogue an...|      0|     1908|     \\N|           120|   Adventure,Fantasy|\n",
      "+---------+---------+--------------------+--------------------+-------+---------+-------+--------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "movies =  \"\"\" SELECT *\n",
    "              FROM FilmTitle\n",
    "              WHERE titleType = 'movie'\n",
    "         \"\"\"\n",
    "movies = spark.sql(movies)\n",
    "movies.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540e60bf",
   "metadata": {},
   "source": [
    "Create a new table that unify idenfiers for the item (nconst<->actors) and for the basket (tconst<->movies) with a join:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17767e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|   tconst|              nconst|\n",
      "+---------+--------------------+\n",
      "|tt0002591|[nm0029806, nm050...|\n",
      "|tt0003689|[nm0910564, nm052...|\n",
      "|tt0004272|[nm0368875, nm009...|\n",
      "|tt0004336|[nm0268437, nm081...|\n",
      "|tt0005209|[nm0593671, nm039...|\n",
      "+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "B =  \"\"\"  SELECT Job.tconst, Job.nconst\n",
    "          FROM Job LEFT JOIN FilmTitle ON Job.tconst = FilmTitle.tconst\n",
    "          WHERE (Job.category == 'actor' OR Job.category == 'actress') AND FilmTitle.titleType == 'movie'\n",
    "\"\"\"\n",
    "a = spark.sql(B)\n",
    "baskets = a.groupBy('tconst').agg(collect_set('nconst').alias('nconst'))\n",
    "baskets.createOrReplaceTempView('baskets')\n",
    "baskets.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc7ec3",
   "metadata": {},
   "source": [
    "Code line necessary to create an RDD:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccfa7ca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['nm0029806', 'nm0509573'], ['nm0910564', 'nm0527801', 'nm0399988', 'nm0101071', 'nm0694718', 'nm0728289', 'nm0585503'], ['nm0368875', 'nm0092665', 'nm0492302', 'nm0445507', 'nm0776747', 'nm0383278', 'nm0192062', 'nm0285643', 'nm0793189']]\n"
     ]
    }
   ],
   "source": [
    "act_bask = baskets.select('nconst').rdd.flatMap(list)\n",
    "print(act_bask.collect()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1895758e",
   "metadata": {},
   "source": [
    "Here two dictionaries to translate identifiers to readeable names and titles based on the original datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2bf9b650-ee85-4fcf-9b87-6b5861d0306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Key_act = {row['nconst']:row['primaryName'] for row in (NamesCast.select(\"nconst\",\"primaryName\")).collect()}\n",
    "Key_title={row['tconst']:row['primaryTitle'] for row in (FilmTitle.select(\"tconst\",\"primaryTitle\")).collect()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c80476",
   "metadata": {},
   "source": [
    "Setting the threshold after which an itemset is consider frequent based on a row count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c181bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "393656 118.09679999999999\n"
     ]
    }
   ],
   "source": [
    "count = act_bask.count()\n",
    "threshold = count*0.0003\n",
    "print(count, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e723cd40",
   "metadata": {},
   "source": [
    "To build the A-priori algorithm PySpark native function are used:\n",
    "flatMap transform rdd to list, map maps the item and its frequency inside a tuple. ReduceByKey uses nconst to aggregate the frequency while filter filters nconst according to frequency and threshold. The procedure is used on both singletons and couples. The input of apriori are an rdd and the threshold used in the filter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80cc890a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apriori (rdd, threshold):\n",
    "  \n",
    "  singleton = rdd.flatMap(list).map(lambda item: (item , 1)).reduceByKey(lambda actor1, actor2: actor1+actor2).filter(lambda item: item[1] >= threshold ) \n",
    "\n",
    "  freq_act = singleton.map(lambda item : (item[0])) # all the pair of frequent items \n",
    "\n",
    "  pairs_of_freq_act = list(itertools.combinations(freq_act.toLocalIterator(),2)) # finding all the possible pairs\n",
    "  \n",
    "  table_of_pairs = rdd.flatMap(lambda x : [(tuple(item), 1) for item in pairs_of_freq_act if set(list(item)).issubset(set(x))]).cache() \\\n",
    "                  .reduceByKey(lambda actor1, actor2: actor1+actor2) \\\n",
    "                  .filter(lambda item: item[1] >= threshold)\n",
    "\n",
    "  return (table_of_pairs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb42d65",
   "metadata": {},
   "source": [
    "The A-Priori algorithm is run on the dataset and converted to a dataframe for better visualization: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b37ed97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+----------------+\n",
      "|Pairs of actors and actresses|Number of movies|\n",
      "+-----------------------------+----------------+\n",
      "|         {nm0623427, nm000...|             237|\n",
      "|         {nm0006982, nm061...|             122|\n",
      "|         {nm0006982, nm041...|             162|\n",
      "|         {nm0046850, nm000...|             169|\n",
      "|         {nm2082516, nm064...|             147|\n",
      "|         {nm2373718, nm064...|             126|\n",
      "+-----------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "AprioriResults = apriori(act_bask, threshold)\n",
    "result = spark.createDataFrame(AprioriResults).toDF(\"Pairs of actors and actresses\", \"Number of movies\")\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3f5931",
   "metadata": {},
   "source": [
    "Time required to run the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8f100d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.999507904052734\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "apriori(act_bask, threshold)\n",
    "APrioriTime = time.time()-startTime\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57577628-1208-45e8-8c43-4df98794391d",
   "metadata": {},
   "source": [
    "Applying translating dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8fafb931-a422-401d-a545-5d80b51ac183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prem Nazir\n",
      "nm0623427\n",
      "Adoor Bhasi\n",
      "nm0006982\n",
      "('nm0623427', 'nm0006982')\n"
     ]
    }
   ],
   "source": [
    "print(Key_act[(result.select('Pairs of actors and actresses')).collect()[0][0][0]])\n",
    "print((result.select('Pairs of actors and actresses')).collect()[0][0][0])\n",
    "print(Key_act[(result.select('Pairs of actors and actresses')).collect()[0][0][1]])\n",
    "print((result.select('Pairs of actors and actresses')).collect()[0][0][1])\n",
    "print((result.select('Pairs of actors and actresses')).collect()[0][0][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ada53d",
   "metadata": {},
   "source": [
    "## FP Growth Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff1cbc0",
   "metadata": {},
   "source": [
    "FP Growth Algorithm is run using the library and calculatin the running time all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b1475fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.464464664459229\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.fpm import FPGrowth\n",
    "startTime = time.time()\n",
    "fp = FPGrowth(itemsCol = \"nconst\", minSupport = 0.0003)\n",
    "startTime = time.time()\n",
    "FPGrowth_model = fp.fit(baskets)\n",
    "FPGrowthTime = time.time()- startTime\n",
    "print(time.time()- startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825a2d2",
   "metadata": {},
   "source": [
    "We display the 10 most frequent items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2abc61f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|      items|freq|\n",
      "+-----------+----+\n",
      "|[nm1388202]| 153|\n",
      "|[nm0430646]| 120|\n",
      "|[nm0103977]| 798|\n",
      "|[nm0006982]| 585|\n",
      "|[nm0436922]| 152|\n",
      "|[nm0408381]| 120|\n",
      "|[nm0648803]| 565|\n",
      "|[nm0405977]| 152|\n",
      "|[nm0576495]| 120|\n",
      "|[nm0579663]| 120|\n",
      "+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FPGrowth_model.freqItemsets.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2ed18dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FPGrowth_model= FPGrowth_model.freqItemsets\n",
    "FPGrowth_model.createOrReplaceTempView(\"FPGrowth_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f089b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|               items|freq|\n",
      "+--------------------+----+\n",
      "|[nm0623427, nm000...| 237|\n",
      "|[nm0046850, nm000...| 169|\n",
      "|[nm0419653, nm000...| 162|\n",
      "|[nm2082516, nm064...| 147|\n",
      "|[nm2373718, nm064...| 126|\n",
      "|[nm0619779, nm000...| 122|\n",
      "+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = \"\"\" SELECT *\n",
    "        FROM FPGrowth_model \n",
    "        WHERE size (items) = 2\n",
    "        ORDER BY freq DESC\n",
    "        \"\"\"\n",
    "spark.sql(q).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3ab28873-68a4-4bb9-8254-923804868480",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prem Nazir\n",
      "nm0623427\n",
      "Adoor Bhasi\n",
      "nm0006982\n",
      "['nm0623427', 'nm0006982']\n"
     ]
    }
   ],
   "source": [
    "print(Key_act[(spark.sql(q).select('fpgrowth_model.items')).collect()[0][0][0]])\n",
    "print((spark.sql(q).select('fpgrowth_model.items')).collect()[0][0][0])\n",
    "print(Key_act[(spark.sql(q).select('fpgrowth_model.items')).collect()[0][0][1]])\n",
    "print((spark.sql(q).select('fpgrowth_model.items')).collect()[0][0][1])\n",
    "print((spark.sql(q).select('fpgrowth_model.items')).collect()[0][0][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be43dc2",
   "metadata": {},
   "source": [
    "## The SON Algortihm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e64718",
   "metadata": {},
   "source": [
    "First of all we create an RDD with the ID of the actors using a sample (due to a problem with the RAM: using all the Job table the sparkContext would be shut down) and then we parallelize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba6610dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "basket_list = baskets.select(\"nconst\").rdd.flatMap(list)\n",
    "rdd_actors = sc.parallelize(basket_list.collect(), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85130a9",
   "metadata": {},
   "source": [
    "We assume a minimum support at 200 and subsequently we adjust it using the number of partitions previously created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e212631",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#180\n",
    "Tot_support = 180\n",
    "num_Parts = rdd_actors.getNumPartitions()\n",
    "par_support = Tot_support/num_Parts\n",
    "print(num_Parts)\n",
    "par_support\n",
    "#spark.reducer.maxSizeInFlight = 512 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78627f2",
   "metadata": {},
   "source": [
    "In the cell below we check the possible candidates in each chunk without filtering them and then we put the results into a data frame in order to show them in a pretty way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "deee8af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def son1 (rdd, num_partitions, support) :\n",
    "  tot_possible_cands = sc.parallelize([])\n",
    "  g_rdd_act = rdd.glom().collect()\n",
    "  print(\"starting...\")\n",
    "  for i in range(0, num_partitions):\n",
    "    partition = sc.parallelize(g_rdd_act[i])\n",
    "    acceptable_tables = apriori(partition, support)\n",
    "    Possible_cands_chunks = acceptable_tables.map(lambda item: (item[0], 1))\n",
    "    tot_possible_cands =tot_possible_cands.union(Possible_cands_chunks)\n",
    "  print(\"Completed!\")\n",
    "  return (tot_possible_cands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c362183a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "Son1Result = son1(rdd_actors, num_Parts, par_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4177b0c5-32c5-4c1a-8c46-53fcb21a7410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('nm0623427', 'nm0006982'), 1)]\n"
     ]
    }
   ],
   "source": [
    "#Son_to_show = Son1Result.map(lambda x: x.split( )[1]) \n",
    "print(Son1Result.take(1))\n",
    "#Son1Result_show = Son1Result.toDF()\n",
    "#Son1Result_show.count()\n",
    "#print(Son1Result.collect()[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456c0d83",
   "metadata": {},
   "source": [
    "Here we calculate the running time for the first part of the SON algorithm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "235b45b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting...\n",
      "Completed!\n",
      "11.118002653121948\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "son1(rdd_actors, num_Parts, par_support)\n",
    "SonTime = time.time()-startTime\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80187d90",
   "metadata": {
    "tags": []
   },
   "source": [
    "Finally we create the final, filtered result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f718b3b-6b8c-44d5-8769-af32c647718d",
   "metadata": {},
   "source": [
    "Is necessary to filter according to the presence in the set of items (baskets) with both custom function \"filtering\" and native filter function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "81c52fb5-fc3c-4b88-9ddc-28f61c231fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(rddlist, filt):\n",
    "  for item in filt:\n",
    "    if set(list(item)).issubset(set(rddlist)):\n",
    "      return ((item, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09a21357",
   "metadata": {},
   "outputs": [],
   "source": [
    "def son2 (rdd, tot_possible_cands, support):\n",
    "  act_id_list = (tot_possible_cands.map(lambda item : item[0])).collect()\n",
    "  result_filtered = rdd.map(lambda x : filtering(x, act_id_list)).filter(lambda x: x is not None).cache().reduceByKey(lambda actor1, actor2: actor1 + actor2).filter(lambda item: item[1] >= support)\n",
    "  return (result_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "293fbe8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('nm0623427', 'nm0006982'), 237)]\n",
      "+-----------------------------+----------------+\n",
      "|Pairs of actors and actresses|Number of movies|\n",
      "+-----------------------------+----------------+\n",
      "|         {nm0623427, nm000...|             237|\n",
      "+-----------------------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Son2Result = son2(rdd_actors, Son1Result, Tot_support)\n",
    "#print(Son2Result.take(1))\n",
    "print(Son2Result.collect())\n",
    "Final_results_SON = spark.createDataFrame(Son2Result).toDF(\"Pairs of actors and actresses\",\"Number of movies\")\n",
    "Final_results_SON.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39dee344",
   "metadata": {},
   "source": [
    "Here we calculate the running time for the second part of the SON algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b7c2f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6981103420257568\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "son2(rdd_actors, Son1Result, par_support)\n",
    "SonTime = time.time()-startTime\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fecfe6ec-df5d-4de6-84b5-a569ca2444a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prem Nazir\n",
      "Adoor Bhasi\n"
     ]
    }
   ],
   "source": [
    "print(Key_act[(Final_results_SON.select('Pairs of actors and actresses')).collect()[0][0][0]])\n",
    "print(Key_act[(Final_results_SON.select('Pairs of actors and actresses')).collect()[0][0][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a9c98-4c88-4b03-a5dc-231ac4e83b36",
   "metadata": {},
   "source": [
    "Filtering the FP models result is possible to extract a complete list of singletons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1225bf96-f19b-4a11-877c-dfcc83f5e68a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+\n",
      "|      items|freq|\n",
      "+-----------+----+\n",
      "|[nm0103977]| 798|\n",
      "|[nm0006982]| 585|\n",
      "|[nm0648803]| 565|\n",
      "|[nm0305182]| 506|\n",
      "|[nm0623427]| 438|\n",
      "|[nm0793813]| 411|\n",
      "|[nm0246703]| 391|\n",
      "|[nm0619107]| 387|\n",
      "|[nm0007123]| 381|\n",
      "|[nm7390393]| 355|\n",
      "+-----------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q_1 = \"\"\" SELECT *\n",
    "        FROM FPGrowth_model \n",
    "        WHERE size (items) = 1\n",
    "        ORDER BY freq DESC\n",
    "        \"\"\"\n",
    "spark.sql(q_1).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e38ade0f-3982-43b4-a8bf-69bb68c9ec58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41\n",
      "0.54\n"
     ]
    }
   ],
   "source": [
    "Prob_Bhasi_towards_Nazir = round(237/585, 2)\n",
    "Prob_Nazir_towards_Bhasi = round(237/438, 2)\n",
    "print(Prob_Bhasi_towards_Nazir)\n",
    "print(Prob_Nazir_towards_Bhasi)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cu110.m74",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m74"
  },
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "record_timing": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
